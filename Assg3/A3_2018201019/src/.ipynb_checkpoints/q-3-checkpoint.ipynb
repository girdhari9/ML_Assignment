{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pprint import pprint\n",
    "import seaborn\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import pi\n",
    "from math import exp\n",
    "import math\n",
    "import sys\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(val):\n",
    "    result = 1 / (1 + np.exp(-val+eps))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(data, Q, target, alpha):\n",
    "    for i in range(10000):\n",
    "        arg = np.dot(data, Q)\n",
    "        seg = sigmoid(arg)\n",
    "        loss = (-target * np.log(seg + eps) - (1 - target) * np.log(1 - seg + eps)).mean()\n",
    "        Q = Q - alpha * np.dot(data.T,(seg - target)) / len(target)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(data, target):\n",
    "    return gradientDescent(data, np.zeros(data.shape[1]), target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(test_row,models,unique_labels):\n",
    "    result=[]\n",
    "    for model in models:\n",
    "        normalised = 0\n",
    "        for i in range(len(model)):\n",
    "            normalised += model[i] * data[i]\n",
    "        result.append(sigmoid(normalised))\n",
    "    maxIndex, index, it = -1, 0, 0\n",
    "    for res in result:\n",
    "        if res > maxIndex:\n",
    "            index, maxIndex = it, res\n",
    "        it += 1\n",
    "    return unique_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, models, unique_labels):\n",
    "    res = []\n",
    "    for i in range(len(data)):\n",
    "        result = predict_one(data[i], models, unique_labels)\n",
    "        res.append(result)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "\n",
    "unique_labels = np.unique(np.array(train_data[11]))\n",
    "models=[]\n",
    "for x in unique_labels:\n",
    "    train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "    train_data[11] = np.where(train_data[11] != x, 0, train_data[11])\n",
    "    train_data[11] = np.where(train_data[11] ==x, 1, train_data[11])\n",
    "    X=train_data[[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "    Y=train_data[11]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=3)\n",
    "    xm  = X_train.mean()\n",
    "    xs = X_train.std()\n",
    "    X_train=(X_train-X_train.mean())/X_train.std()\n",
    "    X_test = (X_test-xm)/xs\n",
    "    ones = np.ones([X_train.shape[0],1])\n",
    "    X_train = np.concatenate((ones,X_train),axis=1)\n",
    "    ones1 = np.ones([X_test.shape[0],1])\n",
    "    X_test = np.concatenate((ones1,X_test),axis=1)\n",
    "    models.append(logisticRegression(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for One Vs All  0.5204081632653061\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "X=train_data[[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "Y=train_data[11]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=3)\n",
    "xm  = X_train.mean()\n",
    "xs = X_train.std()\n",
    "X_train=(X_train-X_train.mean())/X_train.std()\n",
    "X_test = (X_test-xm)/xs\n",
    "ones1 = np.ones([X_test.shape[0],1])\n",
    "X_test = np.concatenate((ones1,X_test),axis=1)\n",
    "Y_pred = predict(X_test,models,unique_labels)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy for One Vs All \" ,accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for One Vs All  52.04%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "\n",
    "unique_labels = np.unique(np.array(train_data[11]))\n",
    "unique_labels.sort()\n",
    "l = len(unique_labels)\n",
    "models=[]\n",
    "for i in range(l):\n",
    "    for  j in range(i+1,l):\n",
    "        train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "        train_data = train_data[(train_data[11]==unique_labels[i]) | (train_data[11]==unique_labels[j])]\n",
    "        train_data[11] = np.where(train_data[11] != unique_labels[i], 0, train_data[11])\n",
    "        train_data[11] = np.where(train_data[11] ==unique_labels[i], 1, train_data[11])\n",
    "        X=train_data[[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "        Y=train_data[11]\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=3)\n",
    "        xm  = X_train.mean()\n",
    "        xs = X_train.std()\n",
    "        X_train=(X_train-X_train.mean())/X_train.std()\n",
    "        ones = np.ones([X_train.shape[0],1])\n",
    "        X_train = np.concatenate((ones,X_train),axis=1)\n",
    "        models.append(logisticRegression(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_onevsone(test_row,models,unique_labels):\n",
    "    result=[]\n",
    "    for model in models:\n",
    "        normalised = 0\n",
    "        for i in range(len(model)):\n",
    "            normalised += model[i] * data[i]\n",
    "        result.append(sigmoid(normalised)>0.5)\n",
    "    l = len(unique_labels)\n",
    "    res=[]\n",
    "    x=0\n",
    "    for i in range(l):\n",
    "        for  j in range(i+1,l):\n",
    "            if(result[x]==True):\n",
    "                res.append(unique_labels[i])\n",
    "            else:\n",
    "                res.append(unique_labels[j])\n",
    "            x+=1\n",
    "    uni,c = np.unique(res,return_counts=True)\n",
    "    real_answr = 0\n",
    "    maxi=-1\n",
    "    for i in range(len(c)):\n",
    "\n",
    "        if(c[i]>maxi):\n",
    "            maxi=c[i]\n",
    "            real_answr = uni[i]\n",
    "    return real_answr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ovo(rows,models,unique_labels):\n",
    "    res=[]\n",
    "    for i in range(len(rows)):\n",
    "        res.append(predict_onevsone(rows[i],models,unique_labels))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0.5170068027210885\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input_data/Q3/wine-quality/data.csv',sep=';',header=None)\n",
    "X=train_data[[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "Y=train_data[11]\n",
    "unique_labels = np.unique(Y)\n",
    "unique_labels.sort()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=3)\n",
    "xm  = X_train.mean()\n",
    "xs = X_train.std()\n",
    "X_train=(X_train-X_train.mean())/X_train.std()\n",
    "X_test = (X_test-xm)/xs\n",
    "ones1 = np.ones([X_test.shape[0],1])\n",
    "X_test = np.concatenate((ones1,X_test),axis=1)\n",
    "Y_pred = predict_ovo(X_test,models,unique_labels)\n",
    "Y_pred\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy for One vs All\",accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy for One Vs One 51.7 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
